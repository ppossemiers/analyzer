{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "\n",
    "UID = 'ppossemiers'\n",
    "PWD = 'ikhebeennieuwPasw00rd'\n",
    "USER = 'hibernate'\n",
    "BASE_PATH = '../data/analyzer/'\n",
    "#REPO = 'netty-4.1'\n",
    "REPO = 'hibernate-orm'\n",
    "STOP_LINES = ['*', '//', 'import', 'package', '{', '}', '@']\n",
    "STOP_WORDS = ['abstract','continue','for','new','switch','assert','default','goto','synchronized',\n",
    "            'boolean','do','if','private','this','break','double','implements','protected','throw',\n",
    "            'byte','else','public','throws','case','enum','instanceof','return','transient',\n",
    "            'catch','extends','int','short','try','char','final','interface','static','void',\n",
    "            'class','finally','long','strictfp','volatile','const','float','native','super','while'\n",
    "            'true','false','null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports and directives\n",
    "\n",
    "%matplotlib inline\n",
    "from pyspark import SparkContext\n",
    "import os, re, requests, zipfile, json, operator\n",
    "from StringIO import StringIO\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, HashingTF, IDF\n",
    "import matplotlib.pyplot as plt\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repo_url = 'https://github.com/' + USER + '/' + REPO + '/archive/master.zip'\n",
    "dir_name = BASE_PATH + REPO + '/'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "    zip = requests.get(repo_url)\n",
    "    if zip.ok:\n",
    "        zipdata = StringIO()\n",
    "        zipdata.write(zip.content)\n",
    "\n",
    "        with zipfile.ZipFile(zipdata, 'r') as z:\n",
    "            z.extractall(dir_name)\n",
    "    \n",
    "commits = []\n",
    "stats = requests.get('https://api.github.com/repos/' + USER + '/' + REPO \n",
    "                                     + '/commits', auth=HTTPBasicAuth(UID, PWD))\n",
    "\n",
    "if stats.ok:\n",
    "    stats_json = json.loads(stats.text or stats.content)\n",
    "    for commit in stats_json:\n",
    "        try:\n",
    "            sha = requests.get('https://api.github.com/repos/' + USER + '/' + REPO \n",
    "                                    + '/commits/' + commit['sha'], auth=HTTPBasicAuth(UID, PWD))\n",
    "            if sha.ok:\n",
    "                files_json = json.loads(sha.text or sha.content)\n",
    "                for f in files_json['files']:\n",
    "                    file_name = f['filename'][f['filename'].rfind('/') + 1:]\n",
    "                    if file_name.endswith('.java'):\n",
    "                        commits.append(file_name)\n",
    "        except:\n",
    "            print 'No key present'\n",
    "            \n",
    "commit_dict = dict((x, commits.count(x)) for x in set(commits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_files(directory):\n",
    "    for path, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            yield os.path.join(path, f)\n",
    "\n",
    "def sanitize(contents):\n",
    "    lines = contents.split('\\n')\n",
    "\n",
    "    # remove stop lines\n",
    "    for stop_line in STOP_LINES:\n",
    "        lines = [line.lower().lstrip().replace(';', '') for line in lines if stop_line not in line and line <> '']\n",
    "\n",
    "    # remove stop words\n",
    "    for stop_word in STOP_WORDS:\n",
    "        # replace() doesn't work because variables get mangled\n",
    "        lines = [re.sub(r'\\b%s\\b' % stop_word, '', line) for line in lines]\n",
    "    \n",
    "    # remove operators\n",
    "    lines = [line.replace('=', '').replace('+', '')\n",
    "             .replace('-', '').replace('>', '')\n",
    "             .replace('<', '').replace('|', '')\n",
    "             .replace('%', '') for line in lines]\n",
    "    \n",
    "    # join all lines into one string\n",
    "    return ' '.join(lines)\n",
    "\n",
    "def count_dependencies(src):\n",
    "    names, codes = zip(*src)\n",
    "    deps = {e : 0 for i, e in enumerate(src)}\n",
    "\n",
    "    for name in names:\n",
    "        for code in codes:\n",
    "            if name in code:\n",
    "                deps[name] += 1\n",
    "    \n",
    "    # sort by amount of references\n",
    "    #return sorted(deps.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return {k: v for k, v in deps.iteritems() if v > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = []\n",
    "# get all java source files\n",
    "src_files = [f for f in all_files(BASE_PATH + REPO) if f.endswith('.java')]\n",
    "\n",
    "for n in src_files:\n",
    "    # create tuple of classname and contents of file\n",
    "    # plain read is used to avoid broadcasting the spark context\n",
    "    joined.append((os.path.basename(n).split('.')[0], open(n, 'r').read()))\n",
    "\n",
    "#deps = count_dependencies(joined)\n",
    "deps = count_dependencies([('a', '1'), ('b', '2')])\n",
    "\n",
    "# show top referenced classes\n",
    "refs = sorted(deps.values(), reverse=True)[:25]\n",
    "lbls = sorted(deps, key=deps.get, reverse=True)[:25]\n",
    "# make plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "fig.suptitle('Top 25 referenced classes', fontsize=15)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.barh(y_pos, refs, align='center', alpha=0.4, color='darksalmon')\n",
    "#set ticks\n",
    "y_pos = np.arange(len(lbls)) + 0.5\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(lbls)\n",
    "ax.set_xlabel('References')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_rdd = sc.parallelize(joined)\n",
    "# make tuple of (class_name, sanitized code) for each source file\n",
    "sanitized = joined_rdd.map(lambda tup: (tup[0], sanitize(tup[1])))\n",
    "\n",
    "sanitized_df = sqlContext.createDataFrame(sanitized, ['label', 'code'])\n",
    "tokenizer = Tokenizer(inputCol='code', outputCol='words')\n",
    "words_df = tokenizer.transform(sanitized_df)\n",
    "\n",
    "#for words_label in words_df.select('words', 'label').take(2):\n",
    "#    print(words_label)\n",
    "\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=5)\n",
    "featurizedData = hashingTF.transform(words_df)\n",
    "\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "for features_label in rescaledData.select('features', 'label').take(3):\n",
    "    print(features_label)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
